{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d08fd5f-01d4-4fd5-a649-585daa03f258",
   "metadata": {},
   "source": [
    "# Exploring GenAI in Research and Evaluation\n",
    "\n",
    "## Using Large Language Models (LLMs) for making sense of large volumes of unstructured qualitative data: building a local-RAG. \n",
    "\n",
    "[Simone Lombardini](https://www.simonelombardini.com/home-page) - November 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f3c88-7111-49f5-9258-8b5b50a148bf",
   "metadata": {},
   "source": [
    "As artificial intelligence continues to evolve, researchers and evaluators are increasingly asking how these tools can enhance their work without compromising the rigour and ethics. Over the past months, I've been exploring how generative AI tools can be applied to evaluation practice. \n",
    "\n",
    "In this short post, I’ll share my experience in using Large Language Models (LLMs) to make sense of large volumes of unstructured qualitative data.\n",
    "\n",
    "In particular, I’ve asked myself:\n",
    "1) How can I use Large Language Models (LLMs) to interpret large amounts of unstructured qualitative data? \n",
    "2) How can I do this while maintaining data security and confidentiality of the information in the text?\n",
    "   \n",
    "Here's what I've learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073974c3-5b00-4be8-8a09-293d2565ef0f",
   "metadata": {},
   "source": [
    "<b><u> What is a Large Language Model?\n",
    "\n",
    "Large Language Models (LLMs), like GPT-4, Claude, and Gemini, are AI systems that can read, summarise, analyse, and write text across a wide range of topics and formats. LLMs can answer questions, summarise documents, analyse and explain complex topics; basically, they work with language in surprisingly human-like ways. \n",
    "\n",
    "These models are trained on vast amounts of text by learning patterns about how language works, grammar, facts, reasoning, and writing styles. Their knowledge depends on the information they’ve been trained on or the information they access from the web.  \n",
    "\n",
    "In my example, I wanted to explore the use of LLMs to enquire large amounts of qualitative data collected from Focus Group Discussions (FGDs). In particular, I wanted to make sure the information and responses provided were based on the actual information provided in the FGDs transcripts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb186418-516f-4d80-92a8-0171d4d44425",
   "metadata": {},
   "source": [
    "<b><u> How do I add my own data to these LLMs? \n",
    "\n",
    "This can be done in different ways. It can either:\n",
    "- Retrain a model;  \n",
    "- Provide documents as a context;\n",
    "- Build a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "Retraining a model involves deep, permanent learning of the new material. However, it also requires significant computer power and time, and it becomes obsolete as soon as some new information is added. \n",
    "\n",
    "Providing documents as a context requires sharing information with an external system. It requires having access to AI assistants, which ensures data privacy and integrity. I will cover this in a separate post. \n",
    "\n",
    "I will focus here on the third option, building a RAG system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c35ba-644e-4b14-b0ab-b077bb203ca7",
   "metadata": {},
   "source": [
    "<b><u> What is a RAG?\n",
    "\n",
    "RAG stands for <b>Retrieval-Augmented Generation</b>. It's a technique that combines information retrieval with AI text generation to enhance the accuracy and knowledge of language models.\n",
    "\n",
    "Instead of relying solely on the knowledge baked into a language model during training, RAG works in two steps: First, when you ask a question, the system searches through a database or collection of documents to find relevant information (<b>retrieval</b>). Second, the LLM uses both your question and the retrieved information to generate a response (<b>the generation</b>).\n",
    "\n",
    "RAG addresses several limitations of standard language models. It provides <b>up-to-date information</b>, connecting the model to specific documents rather than being limited to training data. It <b>reduces hallucinations</b> by grounding responses in actual retrieved documents. It is <b>domain-specific</b> by giving the model access to specialised information included in documents, technical manuals, or research papers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ee5c9-bf3b-46d0-abe7-4ba666d7ca16",
   "metadata": {},
   "source": [
    "<b><u> How do I ensure that no information in my transcript documents is ever shared with external service providers?\n",
    "\n",
    "While a RAG can be implemented by calling an API to external providers, I wanted to make sure that <b>no information was ever shared with external service providers</b>. This is to <b>ensure the security and integrity</b> of the information contained in the transcripts. \n",
    "\n",
    "I therefore set up a <b>local RAG system</b>. The main advantage of a local RAG is that all the documents and data stay on my machine, and nothing is sent to external servers. Once set up, it even works offline and does not require API costs or usage fees. This is particularly popular for businesses handling confidential data or individuals who prioritise privacy. \n",
    "\n",
    "The main disadvantage, however, is that the size of the model that can be run on the RAG is given by the hardware available on your machine. Without any hardware investment, I was able to run only small models, which may be less capable than cloud-based options like GPT-4 or Claude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6d2df-41e8-426f-b5d4-6dd7a80c9475",
   "metadata": {},
   "source": [
    "<b><u> How does a local RAG system work? \n",
    "\n",
    "This usually involves: \n",
    "1) <b>Local language model</b>: Open-source models that you run on your own hardware (like Llama, Mistral, or Phi); \n",
    "2) <b>Vector database</b>: Tools to store your document embeddings locally (like ChromaDB, FAISS, or Qdrant); \n",
    "3) <b>Document processing</b>: Scripts to chunk and embed your documents into the vector database; \n",
    "4) <b>Orchestration</b>: Frameworks like LangChain or LlamaIndex to connect everything together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f0929-9e81-42d1-bea7-5c0c81879fad",
   "metadata": {},
   "source": [
    "<b> How do I run a LLM on my own machine?\n",
    "\n",
    "To do this, I used [Ollama](https://ollama.com/). Ollama is a free and open-source AI platform that allows users to run LLMs directly on their devices. It allows you to download and install several different LLMs. Each model has its own advantages and disadvantages. The choice of which model to use depends upon several factors. I will explore in a separate post how the different models I used performed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad1ecf-838d-4c18-8018-860f32150057",
   "metadata": {},
   "source": [
    "<b> What is a vector database?\n",
    "\n",
    "A vector database is a specialized database designed to store and search embeddings. While traditional databases are great at exact matches (for example, find all users where name = 'John', they can't efficiently answer questions such as \"<i>Find examples of challenges raised by the participants in the focus group discussion</i>\". Vector databases solve this problem.\n",
    "\n",
    "When you ask a question, they first convert your question to an <b>embedding</b>. The vector database calculates the distance to all stored vectors. It then returns the closest matches. Think of it like: \"<i>Find the five documents whose meaning is closest to my question</i>\".\n",
    "Popular vector databases for local/self-hosted RAGs include <b>ChromaDB</b> (simple, popular for local RAG), <b>FAISS</b> (Facebook's library, very fast), <b>Qdrant</b> (feature-rich with good performance) and <b>weaviate</b> (open-source with lots of features). \n",
    "\n",
    "In my example, I have used ChromaDB. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02920b0f-5380-441a-ac4b-741aaaf160dc",
   "metadata": {},
   "source": [
    "<b> What are the embeddings?\n",
    "\n",
    "Embeddings are a way to convert text (or images, audio, etc.) into lists of numbers that capture the <i>meaning</i> of that content. These numbers allow computers to understand and compare semantic similarity. Instead of just seeing words as text, embeddings represent them as points in a multi-dimensional space. Words or phrases with similar meanings end up close together in this space. Embeddings let AI understand that \"happy\" is closer to \"joyful\" than to \"sad\", something keyword matching could never do.\n",
    "\n",
    "Actual embeddings typically have: <b>384 dimensions</b>: Smaller, faster models; <b>768 dimensions</b>: Common size (BERT, sentence-transformers); <b>1536 dimensions</b>: OpenAI's embeddings. \n",
    "\n",
    "In my example, I have chosen <b>nomic-embed-text</b>, which is an open-source embedding model created by Nomic AI. It's become quite popular for local RAG systems because it offers excellent performance while being fully open and runnable on your own hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a4a42-2b43-4c96-a55d-f63d607da566",
   "metadata": {},
   "source": [
    "<b> What is a LangChain?\n",
    "\n",
    "LangChain is an open-source framework that helps developers build applications powered by large language models (LLMs). Think of it as a toolkit that simplifies connecting AI models to other data sources and tools.\n",
    "\n",
    "For this step I actually used [LangFlow](https://www.langflow.org/). LangFlow is essentially a graphical interface for LangChain. It provides you a visual interface to allow you to drag components around and generate LangChain code behind the scenes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d2ce2-7a0e-485f-973d-811582d3374e",
   "metadata": {},
   "source": [
    "<b><u> How does a local RAG actually looks  like in LangFlow\n",
    "\n",
    "Once downloaded LangFlow and built the local RAG which has basically two main components. A document ingestion component and a query/response component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4e8aa-7ef5-4f3b-8887-35b8b4cb85d5",
   "metadata": {},
   "source": [
    "A <u>document ingestion</u> component includes: \n",
    "- <b>File loader</b>: Points to text file(s);\n",
    "- <b>Text splitter</b>: Breaks documents into smaller chunks of 1000 words;\n",
    "- <b>Embedding model</b>: Converts text to vectors (nomic-embed-text);\n",
    "- <b>Vector store</b>: Stores embeddings (ChromaDB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fab57-0150-4236-b354-0d4b228fe9b8",
   "metadata": {},
   "source": [
    "![rag_2](image/rag_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125b34e-38ac-40b6-a97f-1b60c6946ee9",
   "metadata": {},
   "source": [
    "A <u>query/response</u> side:\n",
    "- <b>Chat input</b>: To allow the user to ask questions;\n",
    "- <b>Retriever</b>: Searches your local vector database (Chroma DB);\n",
    "- <b>Prompt</b>: Structures how the question is retrieved and sent to the LLM;\n",
    "- <b>Ollama</b>: Run the LLM locally;\n",
    "- <b>Output node</b>: Displays the answers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe3048a-b182-4a2e-9caf-573f50b79eb3",
   "metadata": {},
   "source": [
    "![rag_1](image/rag_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38065d20-429c-4f0e-905c-2f00ef2451fe",
   "metadata": {},
   "source": [
    "<b><u> How do you run LangFlow?\n",
    "\n",
    "It is advised to run LangFlow in a virtual environment. To do this, open a Terminal and run:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "baa4691d-7145-4fb8-9f0d-a48380588800",
   "metadata": {},
   "source": [
    "uv venv\n",
    "\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "uv pip install langflow\n",
    "\n",
    "uv run langflow run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
